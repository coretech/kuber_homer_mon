apiVersion: v1

kind: ConfigMap
metadata:
  name: telegraf-config
  namespace: telegraf
data:
  telegraf.conf: |+
   [global_tags]

    # Configuration for telegraf agent
    [agent]
      ## Default data collection interval for all inputs
      interval = "10s"
      ## Rounds collection interval to 'interval'
      ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
      round_interval = true

      ## Telegraf will send metrics to outputs in batches of at most
      ## metric_batch_size metrics.
      ## This controls the size of writes that Telegraf sends to output plugins.
      metric_batch_size = 1000

      ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
      ## output, and will flush this buffer on a successful write. Oldest metrics
      ## are dropped first when this buffer fills.
      ## This buffer only fills when writes fail to output plugin(s).
      metric_buffer_limit = 10000

      ## Collection jitter is used to jitter the collection by a random amount.
      ## Each plugin will sleep for a random time within jitter before collecting.
      ## This can be used to avoid many plugins querying things like sysfs at the
      ## same time, which can have a measurable effect on the system.
      collection_jitter = "0s"

      ## Default flushing interval for all outputs. Maximum flush_interval will be
      ## flush_interval + flush_jitter
      flush_interval = "10s"
      ## Jitter the flush interval by a random amount. This is primarily to avoid
      ## large write spikes for users running a large number of telegraf instances.
      ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
      flush_jitter = "0s"

     precision = ""

      ## Logging configuration:
      ## Run telegraf with debug log messages.
      debug = true
      ## Run telegraf in quiet mode (error log messages only).
      quiet = false
      ## Specify the log file name. The empty string means to log to stderr.
      logfile = ""

      ## Override default hostname, if empty use os.Hostname()
      hostname = "telegraf-kuber"
      ## If set to true, do no set the "host" tag in the telegraf agent.
      omit_hostname = true


    ###############################################################################
    #                            OUTPUT PLUGINS                                   #
    ###############################################################################

    # Configuration for sending metrics to InfluxDB
    [[outputs.influxdb]]
      urls = ["http://192.168.24.176:8086"]
      username = "admin"
      password = "1cc@syn3rgyb3am"
      #      database = "health" # required
      database = "gatling" # required
      timeout = "5s"
      namepass = ["fspbx_errors*"]
   
    [[outputs.influxdb]]
      urls = ["http://192.168.24.176:8086"]
      username = "admin"
      password = "1cc@syn3rgyb3am"
      #      database = "health" # required
      database = "gatling" # required
      timeout = "5s"
      namepass = ["fspbx_health*"]
   
    [[inputs.diskio]]

    [[inputs.kernel]]

    [[inputs.mem]]
    [[inputs.processes]]


    [[inputs.swap]]

    [[inputs.system]]

    [[inputs.netstat]]

    [[inputs.net]]

    [[inputs.kafka_consumer]]
       name_override = "fspbx_health"
       brokers = ["192.168.28.16:9091","192.168.29.16:9091","192.168.30.16:9091"]
       topics = ["fspbx_health"]
       consumer_group = "telegraf_metrics_consumers"
       data_format = "value"
       data_type = "string"

    [[inputs.kafka_consumer]]
       name_override = "fspbx_health"
       brokers = ["192.168.28.15:9091","192.168.29.15:9091","192.168.30.15:9091"]
       topics = ["fspbx_health"]
       consumer_group = "telegraf_metrics_consumers"
       data_format = "value"
       data_type = "string"
   
    [[processors.starlark]]

    namepass = ["fspbx_health"]
     source = '''
   load("json.star", "json")
   load("logging.star", "log")
   def apply(metric):
           #               print(metric.fields.get("value"))
               j = json.decode(metric.fields.get("value")) # json
               metrics=[]
               for group in j["storage"]:    #  array of jason elements  to become Metrics
                   new_metric = Metric("host_stats")
                   new_metric.fields["size"] = int(group["size"])
                   new_metric.fields["used"] = int(group["used"])
                   new_metric.fields["available"] = int(group["available"])
                   new_metric.tags[str("filesystem")] = group["filesystem"]
                   new_metric.tags[str("path")] = group["path"]
                   new_metric.tags["server_hostname"] = str(j["server_hostname"])
                   new_metric.fields["uptime"] = int(j["uptime"])
                   new_metric.fields["sessions_max"] = int(j["sessions_max"])
                   new_metric.fields["sessions_active"] = int(j["sessions_active"])
                   new_metric.fields["sessions_older2h"] = int(j["sessions_older2h"])
                   new_metric.fields["sessions_since"] = int(j["sessions_since"])
                   new_metric.fields["sessions_5minpeakmax"] = int(j["sessions_5minpeakmax"])
                   new_metric.fields["sessions_peakmax"] = int(j["sessions_peakmax"])
                   new_metric.fields["cps_active"] = int(j["cps_active"])
                   new_metric.fields["cps_5minpeakmax"] = int(j["cps_5minpeakmax"])
                   new_metric.fields["cps_peakmax"] = int(j["cps_peakmax"])
                   new_metric.fields["cps_max"] = int(j["cps_max"])
                   new_metric.fields["cpu_idle"] = 100-float(j["cpu_idle"])
   
   
                   metrics.append(new_metric)
               return metrics
    '''

    [processors.starlark.constants]
    debug_mode = true

    [[inputs.kafka_consumer]]
       name_override = "fspbx_errors"
       brokers = ["192.168.28.16:9091","192.168.29.16:9091","192.168.30.16:9091"]
       topics = ["fspbx_errors"]
       consumer_group = "telegraf_metrics_consumers"
       data_format = "value"
       data_type = "string"

    [[inputs.kafka_consumer]]
       name_override = "fspbx_errors"
       brokers = ["192.168.28.15:9091","192.168.29.15:9091","192.168.30.15:9091"]
       topics = ["fspbx_errors"]
       consumer_group = "telegraf_metrics_consumers"
       data_format = "value"
       data_type = "string"
   
    [[processors.starlark]]

    namepass = ["fspbx_errors"]
     source = '''
   load("json.star", "json")
   load("logging.star", "log")
   def apply(metric):
               print(metric.fields.get("value"))
               #print(metric.fields.get("value"))
               j = json.decode(metric.fields.get("value")) # json
               metrics=[]
               for group in j:    #  array of jason elements  to become Metrics
                      new_metric = Metric("fspbx_http")
                      if  "hostname" in j:
                         new_metric.tags["server_hostname"] = str(j["hostname"])
                         new_metric.fields["request_time"] = int(j["request_time"])
                         new_metric.tags["status"] = str(j["status"])
                         metrics.append(new_metric)
               return metrics
    '''

    [processors.starlark.constants]
    debug_mode = true
